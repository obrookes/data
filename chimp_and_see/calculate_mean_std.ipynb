{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl18206/anaconda3/envs/dataset-upgrade/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/dl18206/anaconda3/envs/dataset-upgrade/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import einops\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_video\n",
    "from torchvision.transforms.functional import resize\n",
    "from torchvision.transforms import Compose, Resize\n",
    "from panaf.datasets import PanAfFullScene\n",
    "from torchvision.transforms.v2 import ConvertDtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([Resize((224,224)), ConvertDtype(dtype=torch.float32)])\n",
    "videos = glob('/home/dl18206/Desktop/phd/data/panaf/acp/videos/all/**/*.mp4', recursive=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**High-memory method for mean and std calc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std_highmem(videos):\n",
    "    frames = []\n",
    "    for video in tqdm(videos):\n",
    "        video = read_video(video, pts_unit='sec')[0]\n",
    "        video = einops.rearrange(video, 'b h w c -> b c h w ')\n",
    "        video = transform(video)\n",
    "        frames.append(video)\n",
    "\n",
    "    imgs = torch.stack(frames, dim=0).numpy()\n",
    "    imgs = einops.rearrange(imgs, 's t c h w -> (s t) c h w ')\n",
    "\n",
    "    mean_r = imgs[:,0,:,:].mean()\n",
    "    mean_g = imgs[:,1,:,:].mean()\n",
    "    mean_b = imgs[:,2,:,:].mean()\n",
    "\n",
    "    std_r = imgs[:,0,:,:].std()\n",
    "    std_g = imgs[:,1,:,:].std()\n",
    "    std_b = imgs[:,2,:,:].std()\n",
    "\n",
    "    return mean_r, mean_g, mean_b, std_r, std_g, std_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/home/dl18206/anaconda3/envs/dataset-upgrade/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "100%|██████████| 5/5 [00:08<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.4452473 0.46967298 0.45827425\n",
      "Std: 0.31590375 0.31013167 0.29014185\n"
     ]
    }
   ],
   "source": [
    "mean_r, mean_g, mean_b, std_r, std_g, std_b = calculate_mean_std_highmem(videos[:5])\n",
    "print(\"Mean:\", mean_r,mean_g,mean_b)\n",
    "print(\"Std:\", std_r,std_g,std_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(videos):\n",
    "    temp = 0.\n",
    "    mean = 0.\n",
    "    nb_images = 0.\n",
    "    nb_samples = 0.\n",
    "\n",
    "    # First pass to calculate mean\n",
    "    for video in tqdm(videos):\n",
    "        video = read_video(video, pts_unit='sec')[0]\n",
    "        video = einops.rearrange(video, 'b h w c -> b c h w ')\n",
    "        video = transform(video)\n",
    "        nb_images += video.size()[0]\n",
    "        for f in video:\n",
    "            mean += torch.mean(f.to(torch.float32), dim=(1, 2))\n",
    "    mean /= nb_images\n",
    "\n",
    "    # Second pass to calculate std\n",
    "    for video in tqdm(videos):\n",
    "        video = read_video(video, pts_unit='sec')[0]\n",
    "        video = einops.rearrange(video, 'b h w c -> b c h w ')\n",
    "        video = transform(video)\n",
    "        for f in video:\n",
    "            temp += ((f.view(3, -1) - mean.unsqueeze(1)) ** 2).sum(dim=1)\n",
    "            nb_samples += np.prod(f.size()[1:])\n",
    "    std = torch.sqrt(temp/nb_samples)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/home/dl18206/anaconda3/envs/dataset-upgrade/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "100%|██████████| 5/5 [00:08<00:00,  1.63s/it]\n",
      "100%|██████████| 5/5 [00:08<00:00,  1.69s/it]\n"
     ]
    }
   ],
   "source": [
    "mean, std = calculate_mean_std(videos[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.4452, 0.4697, 0.4583])\n",
      "Std: tensor([0.3159, 0.3101, 0.2901])\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset-upgrade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
