{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b674f893-4e22-4af7-ac33-81ec8e899fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl18206/anaconda3/envs/translation_net/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import torch\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from einops import rearrange\n",
    "from torchvision.models.video import r3d_18\n",
    "from pytorchvideo.data import LabeledVideoDataset, UniformClipSampler\n",
    "from pytorchvideo.transforms import create_video_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42efcc9d-103b-442d-b11f-a1ce75c55b8b",
   "metadata": {},
   "source": [
    "**Data Handling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb3ecae-159b-42da-afe8-d0c49e671879",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('annotations/train.csv')\n",
    "val = pd.read_csv('annotations/val.csv')\n",
    "test = pd.read_csv('annotations/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b52ebd-29d1-4760-83fa-1c157de83471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise train dataset\n",
    "prefix = '/home/dl18206/Desktop/phd/data/panaf/acp/videos/all'\n",
    "videos = [(f\"{prefix}/{x['video']}.mp4\", {'video_label': ast.literal_eval(x['label'])}) for x in train[['video', 'label']].to_dict(orient='records')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625e1eca-20a5-4cf1-b496-229275d3d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    To ensure a constant number of samples are retrieved from the dataset we use this\n",
    "    LimitDataset wrapper. This is necessary because several of the underlying videos\n",
    "    may be corrupted while fetching or decoding, however, we always want the same\n",
    "    number of steps per epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(self.dataset_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8288d2e-188e-4e1f-b30f-d0a28f156ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_sampler = UniformClipSampler(clip_duration=15)\n",
    "transform = create_video_transform(\n",
    "    mode='train',\n",
    "    video_key='video',\n",
    "    num_samples=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f1d06-a6f9-412e-a9d8-ca05753eaefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LabeledVideoDataset(\n",
    "    labeled_video_paths=videos,\n",
    "    clip_sampler=clip_sampler,\n",
    "    transform=transform,\n",
    "    decode_audio=False,\n",
    ")\n",
    "dataset = LimitDataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab827c-a8ae-4e4c-b217-b40626db79e3",
   "metadata": {},
   "source": [
    "**Flash**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1250f349-acad-4a8f-86d6-01037d2a0d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from flash.video import VideoClassifier, VideoClassificationData\n",
    "from flash.video.classification.input_transform import VideoClassificationInputTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5689002-39fd-4e36-99c1-92263fbcce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolver(root, file_id):\n",
    "    return os.path.join(root, f\"{file_id}.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "734853b3-fe9b-4753-aeb5-f3f215bb059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('annotations/flash/train.csv')\n",
    "targets = list(train.columns[1:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dee3eebf-4101-45cb-a2e2-b4a3f07cefba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dl18206/anaconda3/envs/translation_net/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py:31: LightningDeprecationWarning: `pytorch_lightning.utilities.apply_func.apply_to_collection` has been deprecated in v1.8.0 and will be removed in v2.0.0. Please use `lightning_utilities.core.apply_func.apply_to_collection` instead.\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "t = VideoClassificationInputTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f355e457-0452-45ef-b423-737e07f8fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/home/dl18206/Desktop/phd/data/panaf/acp/videos/all'\n",
    "datamodule = VideoClassificationData.from_csv(\n",
    "    \"video\",\n",
    "    targets,\n",
    "    train_file=\"annotations/flash/train.csv\",\n",
    "    train_videos_root=data_root,\n",
    "    train_resolver=resolver,\n",
    "    transform=t,\n",
    "    batch_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdc227ec-e78a-45ab-aaaa-fb684bb2bfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 ['camera_reaction', 'tool_use', 'object_carrying', 'bipedal', 'feeding', 'carrying', 'vocalisation', 'climbing', 'aggression', 'travel', 'sex', 'piloerection', 'social_interaction', 'grooming', 'display', 'cross_species_interaction', 'resting', 'no_behaviour']\n"
     ]
    }
   ],
   "source": [
    "print(datamodule.num_classes, datamodule.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6655005c-3233-4325-94c7-58a8f7e672b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(8, 3, 16, 244, 244)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe52a9e1-0fae-45fa-9757-6fb50a5a6a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = r3d_18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c34b236-192f-4852-82b9-776abc862c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c21a735a-82d5-434c-b76d-11b25f68aea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoResNet(\n",
       "  (stem): BasicStem(\n",
       "    (0): Conv3d(3, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n",
       "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=400, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b77190b-5a2e-4d75-9572-b3d49d9deb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "model = r3d_18()\n",
    "model.fc = nn.Linear(in_features=512, out_features=18, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd3f7a88-8528-48c6-8519-d3025c5770cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d108ae5d-e1ab-4fe9-bf56-247bafd902cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 18])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30d4d47e-dc74-41e4-9330-a530ecb763da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d88ebfe1-c515-49ab-a01a-7a91725ddfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [tensor([0, 0]), tensor([0, 1]), tensor([0, 1]), tensor([0, 0]), tensor([0, 1]), tensor([0, 0]), tensor([1, 0]), tensor([1, 0]), tensor([0, 0]), tensor([1, 1]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f036c90-17d1-4761-843d-ef3a00e174ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-72a50b18d6c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e76d5f-ea2b-4070-9ee2-293f3a32a717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
